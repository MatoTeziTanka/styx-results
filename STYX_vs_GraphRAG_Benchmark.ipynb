{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STYX vs GraphRAG + Re-ranking Benchmark (P000)\n",
        "\n",
        "**Purpose:** Validate STYX token reduction against state-of-the-art retrieval: GraphRAG and re-ranking.\n",
        "\n",
        "**Test Matrix:**\n",
        "1. Basic RAG (sentence-transformers + ChromaDB) - baseline\n",
        "2. GraphRAG (Microsoft graph-based retrieval)\n",
        "3. RAG + Re-ranking (cross-encoder)\n",
        "4. GraphRAG + Re-ranking + STYX (full stack)\n",
        "\n",
        "**Key Question:** Does STYX still deliver meaningful compression when retrieval is already optimized?\n",
        "\n",
        "**IMPORTANT:** This benchmark must complete before updating lightspeedup.com or tweet angles."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (takes ~2 min)\n",
        "!pip install tiktoken sentence-transformers chromadb graphrag torch transformers -q\n",
        "!pip install cross-encoder-reranker -q 2>/dev/null || pip install sentence-transformers -q"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except:\n",
        "    GEMINI_API_KEY = input('Enter your Gemini API key: ')\n",
        "\n",
        "os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
        "print(f'API key configured')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import torch\n",
        "\n",
        "enc = tiktoken.get_encoding('cl100k_base')\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch GitHub issues\n",
        "def fetch_github_issues(repo='facebook/react', max_issues=287):\n",
        "    issues = []\n",
        "    page = 1\n",
        "    while len(issues) < max_issues:\n",
        "        url = f'https://api.github.com/repos/{repo}/issues'\n",
        "        params = {'state': 'all', 'per_page': 100, 'page': page}\n",
        "        resp = requests.get(url, params=params)\n",
        "        if resp.status_code != 200:\n",
        "            break\n",
        "        batch = resp.json()\n",
        "        if not batch:\n",
        "            break\n",
        "        issues.extend(batch)\n",
        "        page += 1\n",
        "    return issues[:max_issues]\n",
        "\n",
        "issues = fetch_github_issues()\n",
        "print(f'Fetched {len(issues)} issues')\n",
        "\n",
        "# Prepare documents\n",
        "documents = [\n",
        "    f\"Issue #{i['number']}: {i['title']}\\n{i.get('body', '') or ''}\" \n",
        "    for i in issues\n",
        "]\n",
        "print(f'Prepared {len(documents)} documents')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full context baseline\n",
        "full_context = '\\n\\n---\\n\\n'.join(documents)\n",
        "full_tokens = count_tokens(full_context)\n",
        "print(f'Full context: {full_tokens:,} tokens')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPROACH 1: Basic RAG (sentence-transformers + ChromaDB)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "print('Loading embedding model...')\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print('Creating vector store...')\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection('issues')\n",
        "\n",
        "# Add documents\n",
        "embeddings = embedder.encode(documents, show_progress_bar=True)\n",
        "collection.add(\n",
        "    ids=[str(i) for i in range(len(documents))],\n",
        "    embeddings=embeddings.tolist(),\n",
        "    documents=documents\n",
        ")\n",
        "\n",
        "# Query: typical dev question\n",
        "query = \"How do I handle concurrent state updates in React?\"\n",
        "query_embedding = embedder.encode([query])[0]\n",
        "\n",
        "# Retrieve top-k\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding.tolist()],\n",
        "    n_results=20\n",
        ")\n",
        "\n",
        "rag_context = '\\n\\n---\\n\\n'.join(results['documents'][0])\n",
        "rag_tokens = count_tokens(rag_context)\n",
        "print(f'Basic RAG (top-20): {rag_tokens:,} tokens')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPROACH 2: RAG + Re-ranking (Cross-Encoder)\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "print('Loading cross-encoder for re-ranking...')\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# Get more candidates for re-ranking\n",
        "results_50 = collection.query(\n",
        "    query_embeddings=[query_embedding.tolist()],\n",
        "    n_results=50\n",
        ")\n",
        "\n",
        "# Re-rank\n",
        "pairs = [[query, doc] for doc in results_50['documents'][0]]\n",
        "scores = reranker.predict(pairs)\n",
        "\n",
        "# Sort by re-ranker score and take top-10\n",
        "ranked = sorted(zip(results_50['documents'][0], scores), key=lambda x: x[1], reverse=True)\n",
        "reranked_docs = [doc for doc, score in ranked[:10]]\n",
        "\n",
        "reranked_context = '\\n\\n---\\n\\n'.join(reranked_docs)\n",
        "reranked_tokens = count_tokens(reranked_context)\n",
        "print(f'RAG + Re-ranking (top-10 reranked): {reranked_tokens:,} tokens')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPROACH 3: GraphRAG-style (simplified - entity extraction + graph traversal)\n",
        "# Full GraphRAG requires more setup; this simulates the key insight: \n",
        "# traverse relationships to find structurally relevant documents\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# Build simple entity graph\n",
        "entity_graph = defaultdict(set)\n",
        "doc_entities = {}\n",
        "\n",
        "# Extract entities (React-specific keywords)\n",
        "entity_patterns = [\n",
        "    r'useState', r'useEffect', r'useRef', r'useMemo', r'useCallback',\n",
        "    r'Suspense', r'Concurrent', r'Server Component', r'hydration',\n",
        "    r'reconcil', r'fiber', r'scheduler', r'batch', r'transition'\n",
        "]\n",
        "\n",
        "for idx, doc in enumerate(documents):\n",
        "    entities = set()\n",
        "    for pattern in entity_patterns:\n",
        "        if re.search(pattern, doc, re.IGNORECASE):\n",
        "            entities.add(pattern.lower())\n",
        "    doc_entities[idx] = entities\n",
        "    for entity in entities:\n",
        "        entity_graph[entity].add(idx)\n",
        "\n",
        "# Query entities\n",
        "query_entities = {'concurrent', 'state', 'batch', 'usestate'}\n",
        "\n",
        "# Graph traversal: get docs with matching entities + their neighbors\n",
        "relevant_docs = set()\n",
        "for entity in query_entities:\n",
        "    for pattern in entity_patterns:\n",
        "        if entity in pattern.lower():\n",
        "            relevant_docs.update(entity_graph.get(pattern.lower(), set()))\n",
        "\n",
        "# Add documents that share entities with relevant docs (1-hop)\n",
        "expanded_docs = set(relevant_docs)\n",
        "for doc_idx in list(relevant_docs)[:10]:\n",
        "    for entity in doc_entities.get(doc_idx, set()):\n",
        "        expanded_docs.update(list(entity_graph.get(entity, set()))[:5])\n",
        "\n",
        "graphrag_docs = [documents[i] for i in sorted(expanded_docs)[:15]]\n",
        "graphrag_context = '\\n\\n---\\n\\n'.join(graphrag_docs)\n",
        "graphrag_tokens = count_tokens(graphrag_context)\n",
        "print(f'GraphRAG-style (entity graph): {graphrag_tokens:,} tokens')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPROACH 4: STYX Extraction\n",
        "def styx_extract(documents):\n",
        "    decisions = []\n",
        "    constraints = []\n",
        "    tensions = []\n",
        "    anti_patterns = []\n",
        "    \n",
        "    decision_kw = ['decided', 'will', 'must', 'should', 'approved', 'merged', 'implemented', 'fixed']\n",
        "    constraint_kw = ['cannot', 'must not', 'blocked', 'requires', 'depends', 'breaking']\n",
        "    tension_kw = ['vs', 'tradeoff', 'alternative', 'instead', 'conflict', 'disagree', 'but']\n",
        "    anti_pattern_kw = ['don\\'t', 'avoid', 'deprecated', 'wrong', 'mistake', 'bug', 'regression']\n",
        "    \n",
        "    for doc in documents:\n",
        "        doc_lower = doc.lower()\n",
        "        first_line = doc.split('\\n')[0][:100]\n",
        "        \n",
        "        if any(kw in doc_lower for kw in decision_kw):\n",
        "            decisions.append(first_line)\n",
        "        if any(kw in doc_lower for kw in constraint_kw):\n",
        "            constraints.append(first_line)\n",
        "        if any(kw in doc_lower for kw in tension_kw):\n",
        "            tensions.append(first_line)\n",
        "        if any(kw in doc_lower for kw in anti_pattern_kw):\n",
        "            anti_patterns.append(first_line)\n",
        "    \n",
        "    return {\n",
        "        'decisions': list(set(decisions))[:30],\n",
        "        'constraints': list(set(constraints))[:15],\n",
        "        'tensions': list(set(tensions))[:20],\n",
        "        'anti_patterns': list(set(anti_patterns))[:15]\n",
        "    }\n",
        "\n",
        "# Apply STYX to each retrieval method's output\n",
        "styx_on_rag = styx_extract(results['documents'][0])\n",
        "styx_on_reranked = styx_extract(reranked_docs)\n",
        "styx_on_graphrag = styx_extract(graphrag_docs)\n",
        "\n",
        "def format_styx(state):\n",
        "    return f'''## Decisions ({len(state['decisions'])})\n",
        "{chr(10).join(state['decisions']) if state['decisions'] else 'None'}\n",
        "\n",
        "## Constraints ({len(state['constraints'])})\n",
        "{chr(10).join(state['constraints']) if state['constraints'] else 'None'}\n",
        "\n",
        "## Tensions ({len(state['tensions'])})\n",
        "{chr(10).join(state['tensions']) if state['tensions'] else 'None'}\n",
        "\n",
        "## Anti-Patterns ({len(state['anti_patterns'])})\n",
        "{chr(10).join(state['anti_patterns']) if state['anti_patterns'] else 'None'}'''\n",
        "\n",
        "styx_rag_context = format_styx(styx_on_rag)\n",
        "styx_reranked_context = format_styx(styx_on_reranked)\n",
        "styx_graphrag_context = format_styx(styx_on_graphrag)\n",
        "\n",
        "styx_rag_tokens = count_tokens(styx_rag_context)\n",
        "styx_reranked_tokens = count_tokens(styx_reranked_context)\n",
        "styx_graphrag_tokens = count_tokens(styx_graphrag_context)\n",
        "\n",
        "print(f'STYX on RAG: {styx_rag_tokens:,} tokens')\n",
        "print(f'STYX on Reranked: {styx_reranked_tokens:,} tokens')\n",
        "print(f'STYX on GraphRAG: {styx_graphrag_tokens:,} tokens')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RESULTS\n",
        "print('=' * 70)\n",
        "print('STYX vs GraphRAG + Re-ranking BENCHMARK RESULTS')\n",
        "print('=' * 70)\n",
        "print(f'Dataset: facebook/react ({len(issues)} issues)')\n",
        "print(f'Query: \"{query}\"')\n",
        "print(f'Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "print('=' * 70)\n",
        "print(f'{\"Approach\":<35} {\"Tokens\":>10} {\"Reduction\":>12}')\n",
        "print('-' * 70)\n",
        "print(f'{\"Full Context (baseline)\":<35} {full_tokens:>10,} {\"-\":>12}')\n",
        "print('-' * 70)\n",
        "print(f'{\"Basic RAG (top-20)\":<35} {rag_tokens:>10,} {f\"{100-rag_tokens/full_tokens*100:.0f}%\":>12}')\n",
        "print(f'{\"  + STYX\":<35} {styx_rag_tokens:>10,} {f\"{100-styx_rag_tokens/full_tokens*100:.0f}%\":>12}')\n",
        "print('-' * 70)\n",
        "print(f'{\"RAG + Re-ranking (top-10)\":<35} {reranked_tokens:>10,} {f\"{100-reranked_tokens/full_tokens*100:.0f}%\":>12}')\n",
        "print(f'{\"  + STYX\":<35} {styx_reranked_tokens:>10,} {f\"{100-styx_reranked_tokens/full_tokens*100:.0f}%\":>12}')\n",
        "print('-' * 70)\n",
        "print(f'{\"GraphRAG-style (entity graph)\":<35} {graphrag_tokens:>10,} {f\"{100-graphrag_tokens/full_tokens*100:.0f}%\":>12}')\n",
        "print(f'{\"  + STYX\":<35} {styx_graphrag_tokens:>10,} {f\"{100-styx_graphrag_tokens/full_tokens*100:.0f}%\":>12}')\n",
        "print('=' * 70)\n",
        "print(f'\\nKey Finding: STYX provides additional {100-styx_graphrag_tokens/graphrag_tokens*100:.0f}% reduction ON TOP of GraphRAG')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "results = {\n",
        "    'benchmark': 'STYX vs GraphRAG + Re-ranking',\n",
        "    'dataset': 'facebook/react',\n",
        "    'issues_count': len(issues),\n",
        "    'query': query,\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'results': {\n",
        "        'full_context_tokens': full_tokens,\n",
        "        'basic_rag': {'tokens': rag_tokens, 'with_styx': styx_rag_tokens},\n",
        "        'rag_reranking': {'tokens': reranked_tokens, 'with_styx': styx_reranked_tokens},\n",
        "        'graphrag': {'tokens': graphrag_tokens, 'with_styx': styx_graphrag_tokens},\n",
        "    },\n",
        "    'styx_additional_reduction': {\n",
        "        'on_rag': f\"{100-styx_rag_tokens/rag_tokens*100:.1f}%\",\n",
        "        'on_reranking': f\"{100-styx_reranked_tokens/reranked_tokens*100:.1f}%\",\n",
        "        'on_graphrag': f\"{100-styx_graphrag_tokens/graphrag_tokens*100:.1f}%\"\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('styx_vs_graphrag_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print('Results saved to styx_vs_graphrag_results.json')\n",
        "print(json.dumps(results, indent=2))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}